

####  

------

#### 李宏毅强化学习



![image-20220902213057100](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902213057100.png)

![image-20220902213741453](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902213741453.png)

![image-20220902213925064](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902213925064.png)

![image-20220902214407934](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902214407934.png)

- Reward由Sample产生，具有一定的随机性；
- 类比于GAN，Actor为生成器，Env和Reward为鉴别器；

![image-20220902222354872](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902222354872.png)

- L = Cross-entropy：
    - 采取这一行动：`L = Cross-entropy -> argmin L`；
    - 不采取这一行动：`L = - Cross-entropy -> argmin L`；

![image-20220902223143367](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902223143367.png)

![image-20220902223417554](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902223622193.png)

![image-20220902223832623](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220902223832623.png)

- 只考虑到当前reward，没有考虑到长期；

![image-20220904232029260](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220904232029260.png)



**强化学习的Policy：**

![image-20220904232556207](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220904232556207.png)

- 越往后，对`s1`的影响越小 ；

![image-20220904232757018](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220904232757018.png)

-  好和坏是相对的，因此要做标准化；

![image-20220904233845039](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220904233845039.png)

- 与监督训练不同，RL的数据及其标注都在训练过程中：

![image-20220904234121724](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220904234121724.png)

![image-20220904234227064](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220904234227064.png)

![image-20220904234409489](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.assets/image-20220904234409489.png)