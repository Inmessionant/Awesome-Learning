#  Convolutional Neural Networks



------

#### ✅Inception系列

**Inception使用split-transform-merge策略把multi-scale filter生成的不同感受野的特征融合到一起，**有利于识别不同尺度的对象；

- v1：1 x 1 3 x 3 5 x 5 不同感受野；

- v2：引入BN；

- v3：两个3 x 3代替一个5 x 5，3 x 3 = 1 x 3 + 3 x 1；

- v4：引入残差连接；

- **Xception：**
    - **Xception中的深度可分离卷积，与ResNext模块类似”： ->  训练收敛的速度更快，精度更高**
    - Xception 模块为：$\operatorname{Conv}(1 \times 1)+B N+\operatorname{Re} L U+\operatorname{Depth} \operatorname{conv}(3 \times 3)+B N+R e L U$
    - 普通的深度可分离卷积结构为：$\operatorname{Depthconv}(3 \times 3)+B \mathbf{N}+\operatorname{Conv}(1 \times 1)+B N+\operatorname{ReL} U$


![image-20230109224237244](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20230109224237244.png)

![薰风读论文：ResNeXt 深入解读与模型实现](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/v2-2f96cf9cf046c635e61bf6cdb1008131_1440w.jpg)



------

#### ✅MobileNet系列

- v1：
    - **深度可分离卷积 + ReLU6**：3x3深度卷积， 1x1升维；


- v2：
    - 深度卷积训出来的卷积核有不少是空的：在低维度ReLU使得信息丢失；
    - 使用**Inverted residuals：**1x1先升维，3x3深度卷积， 1x1降维，最后的ReLU6替换为Add；


- v3：

    - 使用NAS  / v2的Inverted residuals / **SE模块 / h-swish激活函数**



------

#### ✅ShuffleNet系列

- v1：
    - **pointwise group convolution**（降低1x1卷积的计算量）
    - **channel shuffle**（解决不同组之间的特征图不通信）
- v2：
    - **平衡输入输出通道**(in = out) 
    - **谨慎使用组卷积** 
    - **避免网络碎片化**(一些操作可以合并，conv+BN) 
    - **减少元素级运算**(Add，ReLU)



------

#### ✅ResNet & DenseNet

|              |          |        |                                      |                |
| ------------ | -------- | ------ | ------------------------------------ | -------------- |
| **ResNet**   | 稀疏连接 | Add    | 训练速度快                           | 参数量相对较多 |
| **DenseNet** | 密集连接 | Concat | 训练速度慢（concat需要频繁读取内存） | 参数量相对较少 |
|              |          |        |                                      |                |

- **DenseNet比传统的卷积网络所需要的参数更少：**

    - **密集连接带来了特征重用**，不需要重新学习冗余的特征图；

    - **维度拼接的操作，带来了丰富的特征信息**，利用更少的卷积就能获得很多的特征图；

- **DenseNet提升了整个网络中信息和梯度的流动，对训练十分有利：**密集连接使得**每一层都可以直接从损失函数和原始输入信号获得梯度**，对于训练更深的网络十分有利；
- **密集连接的网络结构有正则化的效果，能够减少过拟合风险**；
- **对显存需求；**





**ResNet解决了什么问题：**

- **网络性能退化能力 ->  恒等映射：**单纯的堆积网络正确率不升反降：假设一个比较浅的网络已经可以达到不错的效果，那么即使之后堆上去的网络什么也不做，模型的效果也不会变差。然而事实上这却是问题所在，**“什么都不做”恰好是当前神经网络最难做到的东西之一；**
- **有效减少梯度相关性的衰减**：即使BN过后梯度的模稳定在了正常范围内，但梯度的相关性实际上是随着层数增加持续衰减的：对于L层的网络来说，没有残差表示的Plain Net梯度相关性的衰减在$1/ 2^L$  ，而ResNet的衰减却只有 $1 / \sqrt(L)$；

- **稳定梯度**：在输出引入一个输入x的恒等映射，则梯度也会对应地引入一个常数1，这样的网络的确不容易出现梯度值异常，在某种意义上起到了**稳定梯度**的作用；

- **shortcut相加可以实现不同层级特征的组合：**因为浅层容易有高分辨率但是低级语义的特征，而深层的特征有高级语义，但分辨率就很低了；





**ResNet两种结构实现，BottleNeck作用：**

- **两个3x3卷积和一个shortcut**  
- **两个1x1卷积中间加一个3x3卷积，然后再加一个shortcut**
- **BottleNeck作用：降低维度，模型压缩，减少计算量**





**DenseNet和ResNet如何选择：**

​	**在小数据集，DenseNet比ResNet要好，因为小数据集的时候容易产生过拟合，但是DenseNet能够很好的解决过拟合的问题：**对于 DenseNet 抗过拟合的原因有一个比较直观的解释，神经网络每一层提取的特征都相当于对输入数据的一个非线性变换，而随着深度的增加，变换的复杂度也逐渐增加（更多非线性函数的复合），相比于一般神经网络的分类器直接依赖于网络最后一层（复杂度最高）的特征，DenseNet 可以综合利用浅层复杂度低的特征和深层复杂度高的特征，因而更容易得到一个光滑的具有更好泛化性能的决策函数；



------

#### ✅ConvNeXt

```
基于ResNet-50架构，借鉴Swin-T思想来进行优化得到ConvNeXt机构，达到了新的准确度；

ConvNeXt-T: C = (96, 192, 384, 768), B = (3, 3, 9, 3)
ConvNeXt-S: C = (96, 192, 384, 768), B = (3, 3, 27, 3)
ConvNeXt-B: C = (128, 256, 512, 1024), B = (3, 3, 27, 3)
ConvNeXt-L: C = (192, 384, 768, 1536), B = (3, 3, 27, 3)
ConvNeXt-XL: C = (256, 512, 1024, 2048), B = (3, 3, 27, 3)
```



![image-20220918233221394](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918233221394.png)



**训练策略 [小数据集找到training recipe，然后固定使用完整数据集训练]**

- 90 epochs  -> **300 epochs**
- **预训练** AdamW optimizer [learning rate4e-3,weight decay of 0.05, batch size 4096, linear warmup + cosine decaying schedule,Layer Scaleof initial value 1e-6,Exponential Moving Average (EMA) ]
- **微调** AdamW optimizer [learning rate5e-5,,weight decay of 1e-8, cosine learning rate schedule,layer-wise learning rate decay,no warmup,batch size 512]
- **data augmentation** ：MixUp ｜ Cutmix｜RandAugment｜Random Erasing
- **regularization schemes**：Stochastic Depth｜Label Smoothing



**宏观设计**

- **stage ratio：**ResNet-50 [3,4,6,3] -> [3,3,9,3] 

- **patchify：**ResNet-50stem的 7 x 7  stride=2 卷积 + stride=2的3x3 max pooling ->  4 x 4  stride=4卷积，类似于patch操作，得到1/4大小的特征 [对于Swin-T模型，其stem是一个patch embedding layer，实际上就是一个stride=4的4x4 conv；对于ViT模型，其patch size一般较大（eg. 16），只采用一个stride较大的conv来做patch embedding可能会导致训练不稳定，可以将patch embed设计成几个堆叠的stride=2的3 x 3 卷积，无论是在模型效果上，还是在训练稳定性以及收敛速度都更好，而Swin-T的patch size相对较小，不会出现ViT的上述问题]

- **ResNeXt-ify**：depthwise conv [depthwise convolution is similar to the weighted sum operation in self-attention, which operates on a per-channel basis, i.e., only mixing information in the spatial dimension] |  提升通道数 from 64 to 96 

- **Large Kernel Sizes：**7×7 depthwise conv｜Moving up depthwise conv layer [Figure 3 (b) to (c),That is a design decision also evident in Transformers: the MSA block is placed prior to the MLP layers]

- **Inverted Bottleneck：**

  ![image-20220918233230874](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918233230874.png)



**微观设计**

- ReLU -> GeLU & BN -> LN
- Fewer activation functions and  normalization layers [dw conv7x7 + LN + conv 1x1 + GELU + conv 1x1]
- 分离的2 x 2  stride=2卷积下采样 [下采样是放在两个stage之间，但如果直接使用分离下采样会出现训练发散问题，解决的办法是在stem之后，每个下采样层之前以及global avg pooling之后都增加一个LayerNom]

![image-20220918233238722](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918233238722.png)



**代码实现**

```python
class Block(nn.Module):
    r""" ConvNeXt Block. There are two equivalent implementations:
    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)
    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back
    We use (2) as we find it slightly faster in PyTorch
    
    Args:
        dim (int): Number of input channels.
        drop_path (float): Stochastic depth rate. Default: 0.0
        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.
    """
    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv
        self.norm = LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(4 * dim, dim)
        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), 
                                    requires_grad=True) if layer_scale_init_value > 0 else None
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, x):
        input = x
        x = self.dwconv(x)
        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)

        x = input + self.drop_path(x)
        return x
```



------

#### RepVGG



# Transformer



------

#### ✅Transformer

![image-20220918224653971](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918224653971.png)

**Encoder：**

![image-20220912140812361](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912140812361.png)

![image-20220912140452288](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912140452288.png)

![image-20220912140718333](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912140718333.png)



**并行计算（矩阵）：**

![image-20220912141427047](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912141427047.png)

![image-20220912142420754](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912142420754.png)

![image-20220912142616429](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912142616429.png)

![image-20220912142754458](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912142754458.png)



**Multi-head Self-Attention：**

![image-20220912143308197](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912143308197.png)

![image-20220912143358020](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912143358020.png)

![image-20220912143425553](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912143425553.png)

 

**Positional Encoding：**

![image-20220912143838417](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912143838417.png)



**Self-Attention应用：**

![image-20220912144257268](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912144257268.png)

![image-20220912144444206](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912144444206.png)

 ![image-20220912144618118](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912144618118.png)

![image-20220912145315059](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912145315059.png)

![image-20220912145602269](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220912145602269.png) 



**DeCoder：**

![image-20220918223933245](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918223933245.png)

![image-20220918224120740](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918224120740.png)



**Masked Self-Attention：**（只考虑已有）

![image-20220918224822428](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918224822428.png)



**模型要自己考虑输出长度，因此需要一个特殊Token（End）**

![image-20220918225455607](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918225455607.png)

![image-20220918230019878](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918230019878.png)

![image-20220918230218396](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918230218396.png)



**训练过程：**

![image-20220918230835976](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918230835976.png)

![image-20220918230928788](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918230928788.png)

![image-20220918231033040](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220918231033040.png)



------

#### ViT



------

#### Swin Transformer





------

#### ✅Chat GPT & InstructGPT



![image-20221229203938097](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221229203938097.png)

**Chat GPT的四个学习阶段：**

- **学习文字接龙；**

    ![image-20221215222143610](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221215222143610.png)

    ![image-20221215222331117](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221215222331117.png)

    - 文字接龙可以用来回答问题，但是由于GPT每次输出都不同，因此需要人类引导老师文字接龙的方向；

    

- **人类引导老师文字接龙的方向；**（模型1：SFT，Supervised Fine-Tuning）

    ![image-20221215222959661](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221215222959661.png)

    

- **模仿人类老师的喜好；**（模型2：RM，Reward Modeling）

    ![image-20221215223555679](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221215223555679.png)

    - 人类老师标注哪个是比较好的回答，然后训练一个Teacher模型，他能输出标注的回答，即：“台湾最高的山是哪座？玉山”的分数要高于“台湾最高的山是哪座？谁来告诉我呀”；

    

- **用增强式学习向模拟老师学习；**

    ![image-20221215224152761](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221215224152761.png)

    ![image-20221215224115504](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221215224115504.png)



# Generative Adversaial Network



------

#### ✅GAN

![image-20220920233250315](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220920233250315.png)

![image-20220920233926268](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220920233926268.png)

![image-20220920234409233](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220920234409233.png)

![image-20220920234623298](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220920234623298.png)

![image-20220920235118878](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220920235118878.png)

![image-20220920235415708](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220920235415708.png)

![image-20220921000638262](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921000638262.png)

![image-20220921223142947](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921223142947.png)

![image-20220921223425427](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921223425427.png)

![image-20220921223558913](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921223558913.png)

![image-20220921225059294](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921225059294.png) 

**JS 散度的问题，我们可以换一个更好的用于评估相似程度  -> 推土机**

![image-20220921225508938](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921225508938.png)

![image-20220921225657533](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921225657533.png)

![image-20220921230348224](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220921230348224.png)

![image-20220922231755890](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922231755890.png)

- **Model Collapse：生成器的结果来来回回都是那些；**
- **Model Dropping：生成的结果会偏向一个特征；**

![image-20220922232818208](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922232818208.png)

![image-20220922233139989](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922233139989.png)

![image-20220922233603717](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922233603717.png)

![image-20220922233615184](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922233615184.png)

![image-20220922233807853](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922233807853.png)

![image-20220922234147536](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922234147536.png)

![image-20220922234233245](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922234233245.png)

![image-20220922234530889](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922234530889.png)

![image-20220922234634242](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220922234634242.png)

![](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220929230934253.png)

![image-20220929231635782](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20220929231635782.png)



# Self-supervised Learning



![image-20221017151841159](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221017151841159.png)

![image-20221002102342140](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221002102342140.png)

**半监督学习：一部分作为输入，另外一部分作为监督信息；**



#### ✅BERT Series

![image-20221002102954731](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221002102954731.png)

![image-20221002103420800](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221002103420800.png)

![ ](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221002104650532.png)

**BERT在上游使用半监督学习，下游任务使用监督学习；**



![image-20221012215323734](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221012215323734.png)

![image-20221012215619780](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221012215619780.png)

![image-20221012215644403](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221012215644403.png)

![image-20221012222240224](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221012222240224.png)



# Auto-Encoder



![image-20221017152347243](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221017152347243.png)

![image-20221017153219472](%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B.assets/image-20221017153219472.png)
