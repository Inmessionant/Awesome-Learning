##  ########################################



**AI Performance = Data(70%) + Model(CNN、RNN、Transformer、Bert、GPT 20%) + Trick(loss、optimizer、etc 10%)** 



####  调参经验

```
网络：
 1.Backbone和Heads的不同学习率: 因为Backbone和Heads在结构上的差异，使用不同的学习率是可以有效的使得网络达到更好, 更稳定的收敛效果


数据：
 1.数据归一化: 由于白噪声的存在，医学图像、遥感图像则不能简单的归一化到0-1，普通数码照片可以简单的将0-255线性映射到0-1进行归一化
 2.cv2读取图片速度快比Pillow快
 3.Callback: 自己给自己的测试集打Label放进训练


train.py
 01. DropOut2d / DropPath / DropBlock (block size控制大小最好在7x7，keep prob在整个训练过程中从1逐渐衰减到指定阈值)
 02. Batch Normalization / Group Normalization (每组channel为16)
 03. Focal Loss: 对CE loss增加了一个调制系数来降低容易样本的权重值，使得训练过程更加关注困难样本
 04. 多loss加权混合: 保证组合后的损失函数下降不平衡导致的损失函数的倾斜化
 05. OneCycleLR + SGD / Adam （torch.optim.lr_scheduler.ReduceLROnPlateau）
 06. L1 L2正则化 == weight decany
 07. 混合精度FP16: 加快训练速度，提高训练精度
 08. 加速训练pin_memory=true work_numbers=x (卡的数量x4)，data.to(device, no_blocking=True)设置为True后数据直接保存在锁页内存中，后续直接传入cuda，否则需要先从虚拟内存中传入锁页内存中再传入cuda
 09. Warm Up / Early stopping
 10. TTA: 对于测试数据集进行不同方向的预测后将预测的模型进行组合，在不改变模型内部参数的情况下，对效果进行提升 (增加推理时间)
 11. seed（42）随即种子数设置为42
 12. 3x3卷积有利于保持图像性质
 13. ReLU可使用inplace操作减少显存消耗
 14. Label Smoothing: 使得原本的hard-target变为soft-target，让标签分布的熵增大,使网络优化更加平滑,通常用于减少训练的过拟合问题并进一步提高分类性能
 15. With Flooding: 当training loss大于一个阈值时，进行正常的梯度下降；当training loss低于阈值时，会反过来进行梯度上升，让training loss保持在一个阈值附近，让模型持续进行"random walk"


```



#### 深度学习挑战

- **应该更多地关注边缘情况（也就是异常值，或不寻常的情况），并思考这些异常值对预测可能意味着什么：**我们手上有大量的关于日常事务的数据，当前的技术很容易处理这些数据；而对于罕见的事件，我们得到的数据非常少，且目前的技术很难去处理这些数据；
- **我们人类拥有大量的不完全信息推理的技巧，也许可以克服生活中的长尾问题：**但对于目前流行的、更多依赖大数据而非推理的人工智能技术来说，长尾问题是一个非常严重的问题；
- 世上并不只有一种思维方式，因为思维并不是一个整体。相反，**思维是可以分为部分的，而且其不同部分以不同的方式运作：**例如，深度学习在识别物体方面做得相当不错，但在计划、阅读或语言理解方面做得差一些；
- **使用深度学习进行调试非常困难，因为没有人真正理解它是如何工作的，也没有人知道如何修复问题：**大众所知道的那种调试在经典编程环境中并不适用；



#### 有效阅读PyTorch源码

- **项目背景调研 + Paper**；
- 阅读项目说明文档 + **README**；
- 通过文件命名分析：**数据处理、数据加载**部分，通常命名xxx_dataloader.py等；**网络模型**构建部分，通常命名 resnet20.py model.py等；**训练部分**脚本，通常命名为train.py等；**测试部分**脚本，通常命名为test.py eval.py 等；**工具库**，通常命名为utils文件夹；
- **用IDE打开项目**，**找到项目运行的主入口**，阅读入口文件的逻辑，查看调用到了哪些 -  通过IDE的功能跳转到对应类或者函数进行继续阅读，配合代码注释进行分析。一开始可以泛读，大概了解整体流程，做一些代码注释，而后可以精读，找到文章的核心，反复理解核心实现；



## ########################################



## PyTorch



#### 默认梯度累积

- 机器显存小，可以变相增大batchsize；
- weight在不同模型之间交互时候有好处；（动手学习深度学习v2）

```py
accumulation_steps = batch_size // opt.batch_size

loss = loss / accumulation_steps
running_loss += loss.item()
loss.backward()

if ((i + 1) % accumulation_steps) == 0:
	optimizer.step()
	scheduler.step()
	optimizer.zero_grad()
```



------

#### PyTorch提速

- **图片解码**：cv2要比Pillow读取图片速度快
- 加速训练**pin_memory=true / work_numbers=x(卡的数量x4) / prefetch_factor=2 / data.to(device,  no_blocking=True)**
- **DALI库**在GPU端完成这部分**数据增强**，而不是**transform**做图片分类任务的数据增强
- OneCycleLR + SGD / AdamW
- `torch.nn.Conv2d(..., bias=False, ...)`
- DP & DDP 
- 不要频繁在CPU和GPU之间转移数据
- **混合精度训练：**`from torch.cuda import amp`使用`FP16`



------

#### Module & Functional

- **nn.Module**实现的layer是由class Layer(nn.Module)定义的特殊类，**会自动提取可学习参数nn.Parameter**；
- **nn.Functional**中的函数更像是**纯函数**，由def function(input)定义，一般只定义一个操作，其无法保存参数；



- **Module**只需定义 __init__和**forward**，而backward的计算由自动求导机制；
- **Function**需要定义三个方法：**init, forward, backward**（需要自己写求导公式） ；



- **对于激活函数和池化层，由于没有可学习参数，一般使用nn.functional完成，其他的有学习参数的部分则使用nn.Module；**
- 但是**Droupout**由于在训练和测试时操作不同，所以**建议使用nn.Module实现**，它能够通过**model.eval**加以区分；



------

#### Sequential & ModuleList

**区别：**

- **nn.Sequential内部实现了forward函数，而nn.ModuleList则没有实现内部forward函数**；
- **nn.Sequential可以使用OrderedDict对每层进行命名**;
- **nn.Sequential里面的模块按照顺序进行排列的**，所以必须确保前一个模块的输出和下一个模块的输入是一致的；而**nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言**；
- **nn.ModuleList，它是一个储存不同 Module，并自动将每个 Module 的 Parameters 添加到网络之中的容器**；



**nn.Sequential**

- nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的；
- nn.Sequential中可以使用OrderedDict来指定每个module的名字，而不是采用默认的命名方式；
- nn.Sequential内部实现了forward函数；

```python
from collections import OrderedDict

class net_seq(nn.Module):
    def __init__(self):
        super(net_seq, self).__init__()
        self.seq = nn.Sequential(OrderedDict([
                        ('conv1', nn.Conv2d(1,20,5)),
                         ('relu1', nn.ReLU()),
                          ('conv2', nn.Conv2d(20,64,5)),
                       ('relu2', nn.ReLU())
                       ]))
    def forward(self, x):
        return self.seq(x)
net_seq = net_seq()
```



**nn.ModuleList**

- **nn.ModuleList，它是一个储存不同 Module，并自动将每个 Module 的 Parameters 添加到网络之中的容器**：你可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中，而使用 Python 的 list 添加的卷积层和它们的 parameters 并没有自动注册到我们的网络中；
- nn.ModuleList需要手动实现内部forward函数；

```python
class net_modlist(nn.Module):
    def __init__(self):
        super(net_modlist, self).__init__()
        self.modlist = nn.ModuleList([
                       nn.Conv2d(1, 20, 5),
                       nn.ReLU(),
                        nn.Conv2d(20, 64, 5),
                        nn.ReLU()
                        ])

    def forward(self, x):
        for m in self.modlist:
            x = m(x)
        return x

net_modlist = net_modlist()
```



------

#### DataLoader & Sampler & DataSet 

```python
class DataLoader(object):
	# DataLoader.next的源代码，__next__函数可以看到DataLoader对数据的读取其实就是用了for循环来遍历数据
    def __next__(self):
        if self.num_workers == 0:  
            indices = next(self.sample_iter)  # Sampler
            # collate_fn的作用就是将一个batch的数据进行合并操作。默认的collate_fn是将img和label分别合并成imgs和labels，所以如果你的__getitem__方法只是返回 img, label,那么你可以使用默认的collate_fn方法，但是如果你每次读取的数据有img, box, label等等，那么你就需要自定义collate_fn来将对应的数据合并成一个batch数据，这样方便后续的训练步骤
            batch = self.collate_fn([self.dataset[i] for i in indices]) # Dataset遍历数据，self.dataset[i]=(img, label)
            if self.pin_memory:
                batch = _utils.pin_memory.pin_memory_batch(batch)
            return batch
```



- **一般来说PyTorch中深度学习训练的流程是这样的： 1. 创建Dateset ；2. Dataset传递给DataLoader； 3. DataLoader迭代产生训练数据提供给模型；**
- 假设我们的数据是一组图像，每一张图像对应一个index，那么如果我们要读取数据就只需要对应的index即可，即代码中的`indices`，而选取index的方式有多种，有按顺序的，也有乱序的，所以这个工作需要`Sampler`完成，`DataLoader`和`Sampler`在这里产生关系；
- 我们已经拿到了indices，那么下一步我们只需要根据index对数据进行读取即可了，这时`Dataset`和`DataLoader`产生关系；

```
-------------------------------------
| DataLoader												|				
|																		|							
|			Sampler -----> Indices				|  													
|                       |						|	
|      DataSet -----> Data					|
|												|						|			
------------------------|------------                    
												|s						
                        Training
```



```python
class DataLoader(object):
  # DataLoader 的源代码
    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,
                 batch_sampler=None, num_workers=0, collate_fn=default_collate,
                 pin_memory=False, drop_last=False, timeout=0,
                 worker_init_fn=None)
```



DataLoader 的源代码初始化参数里有两种sampler：`sampler`和`batch_sampler`，都默认为`None`；前者的作用是生成一系列的index，而batch_sampler则是将sampler生成的indices打包分组，得到batch的index；

```python
>>>in : list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))
>>>out: [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
```



Pytorch中已经实现的`Sampler`有如下几种：`SequentialSampler` 	`RandomSampler`	 `WeightedSampler` 	`SubsetRandomSampler`,需要注意的是DataLoader的部分初始化参数之间存在互斥关系，这个你可以通过阅读[源码](https://github.com/pytorch/pytorch/blob/0b868b19063645afed59d6d49aff1e43d1665b88/torch/utils/data/dataloader.py#L157-L182)更深地理解，这里只做总结：

- 如果你自定义了`batch_sampler`,那么`batch_size`, `shuffle`,`sampler`,`drop_last`这些参数都必须使用默认值；
- 如果你自定义了`sampler`，那么`shuffle`需要设置为`False`；
- 如果`sampler`和`batch_sampler`都为`None`,那么`batch_sampler`使用Pytorch已经实现好的`BatchSampler`,而`sampler`分两种情况：
  - 若`shuffle=True`,则`sampler=RandomSampler(dataset)`
  - 若`shuffle=False`,则`sampler=SequentialSampler(dataset)`




如何自定义Sampler和BatchSampler：查看源代码其实可以发现，所有采样器其实都继承自同一个父类，即`Sampler`,其代码定义如下：

```python
class Sampler(object):
    r"""Base class for all Samplers.
    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a
    way to iterate over indices of dataset elements, and a :meth:`__len__` method
    that returns the length of the returned iterators.
    .. note:: The :meth:`__len__` method isn't strictly required by
              :class:`~torch.utils.data.DataLoader`, but is expected in any
              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.
    """

    def __init__(self, data_source):
        pass

    def __iter__(self):
        raise NotImplementedError
		
    def __len__(self):
        return len(self.data_source)
```

- 所以你要做的就是定义好`__iter__(self)`函数，不过要注意的是该函数的返回值需要是可迭代的，例如`SequentialSampler`返回的是`iter(range(len(self.data_source)))`；
- 另外`BatchSampler`与其他Sampler的主要区别是它需要将Sampler作为参数进行打包，进而每次迭代返回以batch size为大小的index列表。也就是说在后面的读取数据过程中使用的都是batch sampler；



Dataset定义方式如下：

```python
class Dataset(object):
	def __init__(self):
		...
		
	def __getitem__(self, index):
		return ...
	
	def __len__(self):
		return ...
```

- 面三个方法是最基本的，其中`__getitem__`是最主要的方法，它规定了如何读取数据。但是它又不同于一般的方法，因为它是python built-in方法，其主要作用是能让该类可以像list一样通过索引值对数据进行访问。假如你定义好了一个dataset，那么你可以直接通过`dataset[0]`来访问第一个数据；



------

#### Model.Eval & Torch.No_Grad

- **两者都在Inference时候使用，但是作用不相同：**
  - model.eval() 负责改变batchnorm、dropout的工作方式，如在eval()模式下，dropout是不工作的；
  - torch.no_grad() 会关闭自动求导引擎，节省显存和eval的时间；
- **只进行Inference时，`model.eval()`是必须使用的，否则会影响结果准确性； 而`torch.no_grad()`并不是强制的，只影响运行效率；**



------

#### nn.Linear 和nn.Embedding

- **对torch.tensor([1])做Embedding，可以拿到embedding权重当中的第1号位置的一行（查表操作），Linear则会把你的输入和权重做一个矩阵乘法得到输出**；

- **输入不同：**Embedding输入数字，Linear输入one hot向量；

- **本质相同，nn.Embedding等价于torch.one_hot+nn.linear（bias为0）：**查表的操作本质还是相当于一个one_hot向量和权重矩阵做了一次矩阵乘法；

- 虽然从运算过程来说，nn.Embedding与nn.Linear几乎相同，但是nn.Embdding的层的参数是是直接对应于词的表征的，这和nn.Linear（one-hot向量没有任何的语义信息）还是有本质区别的；

- 习惯上，我们在模型的第一层使用的是Embedding，模型的后续不会再使用Embedding，而是使用Linear；

  

```python
import torch

embedding = torch.nn.Embedding(3, 4)
print(embedding.weight)
print(embedding(torch.tensor([1])))

###############################################
Parameter containing:
tensor([[ 1.6238, -0.0947,  0.1135,  1.0270],
        [ 0.3348,  0.2148, -0.5463, -1.3829],
        [-0.3593, -1.0826, -1.0345, -1.5916]], requires_grad=True)
tensor([[ 0.3348,  0.2148, -0.5463, -1.3829]], grad_fn=<EmbeddingBackward0>)

# nn.Embedding是用来将一个数字变成一个指定维度的向量的，比如数字1变成一个128维的向量，数字2变成另外一个128维的向量，这些128维的向量是模型真正的输入。不过这128维的向量并不是永恒不变的，这128维的向量会参与模型训练并且得到更新，从而数字1和2会有更好的128维向量的表示；
```



------

#### Seed 引发的可复现性陷阱



PyTorch模型训练中的**两种可复现性**：

```
1. 在完全不改动代码的情况下重复运行，获得相同的准确率曲线； ->  固定所有随机数种子
2. 改动有限的代码，改动部分不影响训练过程的前提下，获得相同的曲线； -> 改动的代码没有影响random()的调用顺序
```



**1 第一种情况，我们只需要固定所有随机数种子就行**

计算机一般会使用混合线性同余法来生成伪随机数序列，在我们每次调用rand()函数时就会执行一次或若干次下面的递推公式：
$$
x_{n+1}=\left(a x_n+c\right) \bmod (m)
$$

- 当 a 、 c 和 m 满足一定条件时，可以近似地认为 x 序列中的每一项符合均匀分布，通过 x/m 我们可以得到0到1之间的随机数；
- 这类算法都有一个特点，就是一旦固定了序列的初始值 $x_0$ ，整个随机数序列也就固定了，这个初始值就被我们称作种子：
    - 我们在程序的起始位置设定好随机数种子，程序单次执行中第 n 次调用到rand()得到的数值将会是固定的；
    - **一旦程序中rand()函数的调用顺序固定**，无论程序重复运行多少遍，结果都将是稳定的；
- 在PyTorch中我们一般使用seed_everything固定随机数种子，它调用尽量放在所有import之后，其他代码之前；

```python
def seed_everything(seed):
    torch.manual_seed(seed)       # Current CPU
    torch.cuda.manual_seed(seed)  # Current GPU
    np.random.seed(seed)          # Numpy module
    random.seed(seed)             # Python random module
    torch.backends.cudnn.benchmark = False    # Close optimization
    torch.backends.cudnn.deterministic = True # Close optimization
    torch.cuda.manual_seed_all(seed) # All GPU (Optional)
```



**2 第二种情况，一定要万分确定改动的代码没有影响random()的调用顺序**

**首先要清楚我提到的固定随机数种子对可复现性起作用的前提：rand()函数调用的次序固定。也就是说，假如在某次rand()调用之前我们插入了其他的rand()操作，那这次的结果必然不同！**

```python
>>> import torch
>>> from utils import seed_everything

>>> seed_everything(0)
>>> torch.rand(5)
tensor([0.4963, 0.7682, 0.0885, 0.1320, 0.3074])  #

>>> seed_everything(0)
>>> _ = torch.rand(1)
>>> torch.rand(5)
tensor([0.7682, 0.0885, 0.1320, 0.3074, 0.6341])  # 偏移一位
```



```
问题描述：在固定随机数种子的前提下，你写了一个训练模型的代码，输出了训练的loss和准确率并绘制了图像，突然你想在每轮训练之后再测一下测试准确率，于是小心翼翼地修改了代码，那么问题来了，训练的loss和准确率会和之前一样吗？ 

-> False，每轮训练完再测试准确率会使用for inputs, labels in dataloader，而这个会引入随机函数；
```



模型测试中唯一不确定的就是DataLoader：按照常规设置，训练时一般使用带shuffle的DataLoader，而测试时使用不带shuffle的，我们进行复现：

```python
import torch
from torch.utils.data import TensorDataset, DataLoader
from utils import seed_everything

seed_everything(0)
dataset = TensorDataset(torch.rand((10, 3)), torch.rand(10))
dataloader = DataLoader(dataset, shuffle=False, batch_size=2)
print(torch.rand(5))
# tensor([0.5263, 0.2437, 0.5846, 0.0332, 0.1387])

seed_everything(0)
dataset = TensorDataset(torch.rand((10, 3)), torch.rand(10))
dataloader = DataLoader(dataset, shuffle=False, batch_size=2)
for inputs, labels in dataloader:
    pass
print(torch.rand(5))
tensor([0.5846, 0.0332, 0.1387, 0.2422, 0.8155])
```



**阅读Pytorch中DataLoader的源码可以发现：**`for inputs, labels in dataloader`的`in`先调用后面的迭代器`dataloader`中的`iter()`，每次遍历数据集时DataLoader的`iter()`都会返回一个新的生成器，这个生成器底层有一个`_index_sampler`，shuffle设置为True时它使用`Batch(RandomSampler)`随机采样`batchsize`个数据索引，如果为False则使用`Batch(SequentialSampler)`顺序采样；而这个生成器的基类叫做`BaseDataLoaderIter`，在它的初始化函数中唯一调用了一次随机数函数，用以确定全局随机数种子`base_seed`，且`base_seed`仅使用在其子类`_MultiProcessingDataLoaderIter`中，当我们将`DataLoader`的`worker`数量设置为大于0时，将使用多进程的方式加载数据，在这个子类的初始化函数中会新建`n`个进程，然后将`base_seed`作为进程参数传入：

```python
# 1.
class _BaseDataLoaderIter(object):
    def __init__(self, loader: DataLoader) -> None:
        ...
        # 调用了一次随机
        self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()
        ...
    
    
# 2.self._base_seed在_MultiProcessingDataLoaderIter中使用      
... 
w = multiprocessing_context.Process(
    target=_utils.worker._worker_loop,
    args=(self._dataset_kind, self._dataset, index_queue,
          self._worker_result_queue, self._workers_done_event,
          self._auto_collation, self._collate_fn, self._drop_last,
          self._base_seed, self._worker_init_fn, i, self._num_workers,
          self._persistent_workers))
w.daemon = True
w.start()
...


# 3.
def _worker_loop(dataset_kind, dataset, index_queue, data_queue, done_event,
                 auto_collation, collate_fn, drop_last, base_seed, init_fn, worker_id,
                 num_workers, persistent_workers):
    ...
    seed = base_seed + worker_id # 确定seed
    random.seed(seed)
    torch.manual_seed(seed)
    if HAS_NUMPY:
        np_seed = _generate_state(base_seed, worker_id)
        import numpy as np
        np.random.seed(np_seed)
    ...
```



**按照PyTorch向后兼容的设计理念，这里无论谁继承_BaseDataLoaderIter这个基类，无论子类是否用到base_seed这个种子，随机数函数都是会被调用的，调用关系如下：**

```python
for inputs, labels in DataLoader(...):
    pass
# 2.in操作符会调用如下
DataLoader()
    DataLoader.self.__iter__()
        DataLoader.self._get_iterator()
            _MultiProcessingDataLoaderIter(DataLoader.self)
                _BaseDataLoaderIter(DataLoader.self)
                    _BaseDataLoaderIter.self._base_seed = torch.empty(
                        (), dtype=torch.int64).random_(generator=DataLoader.generator).item()
# 一般来说generator是None，我们不指定，random_没有from和to时，会取数据类型最大范围，这里相当于随机生成一个大整数
```



- 这一问题的解决方案是在每次DataLoader的in操作调用之前都固定一下随机数种子（1.），**但stable()会使训练时每个epoch内部的shuffle规律相同**；
- 之前我们提到shuffle训练集可以减轻模型过拟合，当每个epoch内部第i个batch的内容都对应相同时模型会训不起来，所以，一个简单的技巧，在传入随机数种子的时候加上一个epoch序号（2.）；
- 这时随机数种子的设定和in操作绑定成了类似的原子操作，所有涉及到random()调用的新增代码都不会影响到准确率曲线的复现了；

```python
# 1.
def stable(dataloader, seed):
    seed_everything(seed)
    return dataloader

for inputs, labels in stable(DataLoader(...), seed):
    pass
  
  
# 2.
def stable(dataloader, seed):
    seed_everything(seed)
    return dataloader
  
for epoch in range(MAX_EPOCH):  # training
    for inputs, labels in stable(DataLoader(...), seed + epoch):
        pass
```



## ########################################



## 深度学习



------

#### 局部最小值 & 鞍点（Hessian矩阵）

- 求导，得到导数为0的点，并进行泰勒展开；
- 泰勒展开含二阶导数部分，求其Hessian矩阵及其对应特征值；
- 特征值全部大于0，则为局部最小值；特征值全部小于0，则为局部最大值；特征值有正有负，则为鞍点；

<img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826223004304.png" alt="image-20220826223004304" style="zoom: 50%;" />

<img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826223246364.png" alt="image-20220826223246364" style="zoom:50%;" />



```
**如果正定Hessian矩阵的特征值都差不多，那么梯度下降的收敛速度越快，反之如果其特征值相差很大，那么收敛速度越慢；**
```



一阶梯度下降方法中$\alpha$控制步长：
$$
x_{t+1}=x_{t}-\alpha g, \quad g=f^{\prime}\left(x_{t}\right)                 
$$
二阶方法将函数$f(\theta)$在局部极值点附$f(\theta^*)$近进行二阶Taylor展开，其中$g$为梯度向量，$H$为Hessian矩阵：
$$
f\left(\theta^{*}+\Delta \theta\right) \approx f\left(\theta^{*}\right)+g^{T} \Delta \theta+\frac{1}{2}(\Delta \theta)^{T} H(\Delta \theta)
$$
对上式求导并令其为0，以求在二阶近似原函数的情况下快速求出函数极值点：
$$
\Delta \theta=-H^{-1} g
\Rightarrow x_{t+1}=x_{t}-H^{-1} g, g=f^{\prime}\left(x_{t}\right)
$$
结合两个更新公式可知**Hessian矩阵的特征值控制了更新步长：**$\alpha = H^{-1}$

详细的，对实对称矩阵而言：$H=E \Lambda E^{T}$，其中$E=\left[e_{1} e_{2} \ldots e_{n}\right]$是单位特征向量矩阵，$\Lambda=\operatorname{diag}\left(\left[\lambda_{1} \lambda_{2} \ldots \lambda_{n}\right]\right)$是对应特征值对角矩阵，则：
$$
H^{-1} g=\left(E \Lambda E^{T}\right)^{-1} g=E \Lambda^{-1} E^{T} g=\sum_{i}^{n} \frac{e_{i}^{T} g}{\lambda_{i}} e_{i}
$$
可以看出，这**里控制步长的有对应的Hessian矩阵特征值**，极端的![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+g)则表示![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+%5CLeftrightarrow++1%2F%5Clambda_%7Bi%7D%2C+%5Cvee+i)这种，**若特征值间差异巨大，则有些方向学习缓慢，有些不断波动**（二维情况就是经常看到的那种蛇形曲线...)，这些现象也侧面说明了步长这东西；



------

#### Small Batch & Large Batch

![image-20220826224718603](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826224718603.png)

![image-20220826225430187](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826225430187.png)



------

#### 梯度下降 & 学习率



**一阶方法：**随机梯度下降（SGD）、动量（Momentum）、牛顿动量法（Nesterov动量）、AdaGrad（自适应梯度）、RMSProp（均方差传播）、Adam、Nadam

**二阶方法：**牛顿法、拟牛顿法、共轭梯度法（CG）、BFGS、L-BFGS

**自适应优化：**Adagrad（累积梯度平方）、RMSProp（累积梯度平方的滑动平均）、Adam（带动量的RMSProp，即同时使用梯度的一、二阶矩）

 

------



**一个框架来梳理所有的优化算法：**

- 首先定义待优化参数 $w$,   目标函数$f(w)$,   初始学习率 $\alpha_{\circ}$
- 之后开始进行迭代优化，在每个epoch $\boldsymbol{t}$ :
  - 计算目标函数关于当前参数的梯度： $g_{t}=\nabla f\left(w_{t}\right)$
  - 根据历史梯度计算一阶动量和二阶动量:
    $m_{t}=\phi\left(g_{1}, g_{2}, \cdots, g_{t}\right) ; V_{t}=\psi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$
  - 计算当前时刻的下降梯度： $\eta_{t}=\alpha \cdot m_{t} / \sqrt{V_{t}}$
  - 根据下降梯度进行更新： $w_{t+1}=w_{t}-\eta_{t}$

------



**马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）**

- **基于动量**的方法使得最优化过程看起来像是一个球滚下山的样子
- **SGD**很难突破对称性，一直卡在顶部
- **RMSProp之类**的方法能够看到马鞍方向有很低的梯度（因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进）

------



**SGD（普通更新）**



最简单的沿着负梯度方向改变参数；假设有一个**参数向量x**及其**梯度dx**，那么最简单的更新的形式是：

```python
# 普通更新
x += - learning_rate * dx
```



- SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点； 
- $(\mathrm{W}, \mathrm{b})$ 的每一个分量获得的梯度绝对值有大有小,，一些情况下，将会迫使优化路径变成Z字形状；
- SGD求梯度的策略过于随机，由于上一次和下一次用的是完全不同的Batch数据,，将会出现优化的方向随机的情况；



------

**SGDM（动量更新，解决梯度随机性）**



该方法从**物理角度**上对于最优化问题得到的启发：

从本质上说，动量法就像我们从山上推下一个球，球在滚下来的过程中累积动量，变得越来越快（直到达到终极速度，如果有空气阻力的存在，则$\mu$<1）；同样的事情也发生在参数的更新过程中：**对于在梯度点处具有相同的方向的维度，其动量项增大，对于在梯度点处改变方向的维度，其动量项减小。**因此我们可以得到更快的收敛速度，同时可以减少摇摆。



**也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 mu的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向**；



在SGD中，梯度影响**位置**；

而在SGDM的更新中，物理观点建议**梯度只是影响速度**，然后**速度再影响位置**： 

```python
 # 动量更新
    v = mu * v - (1 - mu) * dx # 与速度融合，mu其物理意义与摩擦系数更一致
    x += v # 与位置融合
    
# mu通常取值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向
```

![image-20220826230230491](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826230230491.png)

![image-20220826230752974](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826230752974.png)



------



**NAG（Nesterov动量）**



SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。



当参数向量位于某个位置 *x* 时，观察上面的动量更新公式，动量部分会通过$mu * v$改变参数向量；

因此，如要计算梯度，那么可以将**未来的近似位置**$ x+mu*v$ 看做是“**向前看**”，这个点在我们一会儿要停止的位置附近。因此，**计算** $ x+mu*v$**的梯度**而不是“旧”位置 *x* 的梯度，使用Nesterov动量，我们就在这个“向前看”的地方计算梯度

```python
x_ahead = x + mu * v
计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)
v = mu * v - learning_rate * dx_ahead
x += v  
```

上面的程序还得计算dx_ahead，通过对 x_ahead = x + mu * v 使用变量变换进行改写，然后用x_ahead而不是x来表示上面的更新，即：实际**存储**的参数向量总是**向前一步版本**。x_ahead 的公式（将其**重新命名为x**）就变成了：

```python
v_prev = v # 存储备份
v = mu * v - learning_rate * dx # 速度更新保持不变，mu=0.9
x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
```



------



**Adagrad**

![image-20220827213002839](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827213002839.png)



------



**RMSprop**



引用自Geoff Hinton的Coursera课程，具体说来，就是它使用了一个**梯度平方的滑动平均**：

```python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

decay_rate=0.9，learning_rate=0.001，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，但**其更新不会让学习率单调变小**； 

- 不累积全部历史梯度，而**只关注过去一段时间窗口的下降梯度**，而指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：
- 历史中梯度权重不同，最近一些梯度具有较大的影响；

![image-20220827213814881](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827213814881.png)



------



**Adam**



**Adam本质上实际是RMSProp+动量**：Adam对每一个参数都计算自适应的学习率：除了像RMSprop一样存储一个历史梯度平方的滑动平均$vt$，Adam同时还保存一个历史梯度的滑动平均$m_t$，类似于动量：

```python
# 根据历史梯度计算一阶动量和二阶动量
m_t = beta1*m + (1-beta1)*dx
v_t = beta2*v + (1-beta2)*(dx**2)

# 当mt和vt初始化为0向量时，发现它们都偏向于0，尤其是在初始化的步骤和当衰减率很小的时候（例如beta1和beta2趋向于1）,通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差
m_hat = m_t / 1 - (beta1 ** t）
v_hat = v_t / 1 - (beta2 ** t)

x += - learning_rate * m_hat / (np.sqrt(v_hat) + eps)  # eps=1e-8, beta1=0.9, beta2=0.999
```

![image-20220827214144011](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827214144011.png)



------



**学习率 Learning Rate**

![image-20220827214838018](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827214838018.png)



**Warm Up 需要在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 Decay：**$\sigma$表示历史统计数据，而在训练开始时这一数据并不准确，所以给它一个较小的学习率(0.001)让它在原地学习，之后数据会逐渐准确，可以将其增加(0.1)，之后使用常见的Learning Rate Decay；

![image-20220827215343432](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827215343432.png)



------

#### Batch Normalization & Dropout



**解决Internal Covariate Shift**：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程；之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近，所以这**导致反向传播时低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**，**而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的分布；**



**优点：**

- **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度；**
- **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题；**
- **起到一定的正则化作用，防止过拟合；**
- **BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定；**



------



$Input: B=\left\{x_{1 \ldots m}\right\} ; \gamma, \beta($ parameters to be learned $)$

$\text { Output }:\left\{y_{i}=B N_{\gamma, \beta}\left(x_{i}\right)\right\} \\$
$$
\begin{array}{r}

\mu_{B} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\
\sigma_{B}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2} \\
\tilde{x}_{i} \leftarrow \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}} \\
y_{i} \leftarrow \gamma \tilde{x}_{i}+\beta
\end{array}
$$

- **BN [B, H, W] 的精髓在于归一之后，使用$\gamma, \beta$作为还原参数，让数据尽可能保留原始的表达能力；**

```
批规范化（Batch Normalization，BN）：在 minibatch维度 上在每次训练iteration时对隐藏层进行归一化
标准化（Standardization）：对输入 数据 进行归一化
正则化（Regularization）：通常是指对 参数 在量级和尺度上做约束，缓和过拟合情况，L1 L2正则化
```



均值的计算，就是在一个批次内将每个通道中的数字单独加起来，再除以 $N*W*H$；举个例子：该批次内有10张图片，每张图片有三个通道RBG，每张图片的高、宽是H、W，那么均值就是计算**10张图片R通道的像素数值总和**除以$10*W*H$，再计算B通道全部像素值总和除以$N*W*H$，最后计算G通道的像素值总和除以$N*W*H$。方差的计算类似；

可训练参数 $\beta,\gamma$的维度等于**张量的通道数**，在上述例子中，RBG三个通道分别需要一个 $\beta和一个\gamma$，所以$\vec{\gamma}, \vec{\beta}$的维度等于3；



------

**基本理论**

![image-20220828125737750](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828125737750.png)

在未进行Normalization前所有的数据都相互独立，但是经过Normalization会使得各个数据间相互关联，每一个数据的改变会改变$\mu和\sigma$进而改变后一层数据；

![image-20220828130410672](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828130410672.png)

$\beta和\gamma$为可学习参，用于还原原始参数分布；

![image-20220828130924049](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828130924049.png)

![image-20220828131321546](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828131321546.png)

![image-20220828133459231](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828133459231.png)

------

**PyTorch中BN**



在PyTorch中**将gamma和beta改叫weight、bias**，使得打印网络参数时候只会打印出weight和bias（PyTorch中只有可学习的参数才称为Parameter）,但是`Net.state_dict()`是有running_mean和running_var的，**因为running_mean和running_var不是可以学习的变量，只是训练过程对很多batch的数据统计;**



BN层的**输出Y与输入X之间的关系**：**Y = (X - running_mean) / sqrt(running_var + eps) * gamma + beta**，其中**gamma、beta为可学习参数（在PyTorch中分别改叫weight和bias），训练时通过反向传播更新**；而**running_mean、running_var则是在前向时先由X计算出mean和var，再由mean和var以动量momentum来更新running_mean和running_var**，所以**在训练阶段，running_mean和running_var在每次前向时更新一次**；在**测试阶段，则通过`net.eval()`固定该BN层的running_mean和running_var，此时这两个值即为训练阶段最后一次前向时确定的值，并在整个测试阶段保持不变；**



**先更新running_mean和running_var，再计算BN；**

```
训练时：
running_mean = (1 - momentum) * running_mean + momentum * mean_cur
running_var = (1 - momentum) * running_var + momentum * var_cur
```

```
测试时：
running_mean = running_mean
running_var = running_var
```



------

**Conv和BN的融合**
$$
\begin{aligned}
y_{\text {conv }} &=w \cdot x+b \\
y_{b n} &=\gamma \cdot\left(\frac{y_{\text {conv }}-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
&=\gamma \cdot\left(\frac{w x+b-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
\hat{w} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot w \\
\hat{b} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot(b-E[x])+\beta \\
y_{b n} &=\hat{w} \cdot x+\hat{b}
\end{aligned}
$$

------

**Dropout**



`torch.nn.Dropout2d(p=0.5, inplace=False)`：input shape: (N, C, H, W)， output shape: (N, C, H, W)



**Dropout在层与层之间加噪声，是一种正则**；**在全连接使用，CNN用BN；**

Dropout 是**在训练过程中以一定的概率的使神经元失活，控制模型复杂度，提高模型的泛化能力，减少过拟合**，而在测试时，应该用整个训练好的模型，因此不需要Dropout；



- **Dropout 在训练时采用**，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险；而在测试时，应该用整个训练好的模型，因此**测试时不需要dropout**；

- **在测试时如果丢弃一些神经元，这会带来结果不稳定的问题：**给定一个测试数据，有时候输出a有时候输出b，结果不稳定，用户可能认为模型预测不准。那么**一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的**。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是`p*x+(1-p)*0=px`，因此测试的时候把这个神经元的权重乘以p可以得到同样的期望；



------

**BN和Dropdout同时使用**



**方差偏移现象**

Dropout 与 Batch Normalization之间冲突的关键是**网络状态切换过程中存在神经方差的不一致行为，这种方差不匹配可能导致数值不稳定；而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能；**



- 假设某层单个神经元在某一Batch的Batch Normalization后输出的期望，方差分别为：$E\left(x_{i}\right)=e$,  $V\left(x_{i}\right)=v$；
- 对于Dropout，训练过程中的计算可以表示为：$x_{d}^{i}=\frac{x_{i}}{p} \delta_{i}$，其中$\delta_{i}$表示服从 概率$p$伯努利分布采样的随机变量；
- 那么在训练阶段经过Dropout后，该Batch的期望和方差分别为：

$$
\begin{gathered}
E\left(x_{d}^{i}\right)=\frac{e}{p} * p+0 *(1-p)=e \\
V\left(x_{d}^{i}\right)=E\left(x_{d}^{i}\right)-E\left(x_{d}^{i}\right)^{2}=\frac{p *\left(V\left(x_{i}\right)+E\left(x_{i}\right)^{2}\right)}{p^{2}}-e^{2}=\frac{v+e^{2}}{p}-e^{2}
\end{gathered}
$$

​	 根据方差和期望的代数关系: $V(x)=E\left(x^{2}\right)-E(x)^{2}$ ：由于 $x_{d}^{i}=\frac{x^{i}}{p} * \delta_{i}$，即：
$$
E\left(x_{d}^{i}\right)=E\left(\left(\frac{x^{i}}{p} * \delta_{i}\right)^{2}\right)=\frac{E\left(\left(x^{i}\right)^{2}\right) * E\left(\delta_{i}^{2}\right)}{p^{2}}=\frac{\left(V\left(x^{i}\right)+E\left(x^{i}\right)^{2}\right) * p}{p^{2}}
$$

- 在Dropout测试阶段，$x_{d}^{i}=\delta_{i}$，不再根据采样概率$p$进行缩放，方差$V\left(x_{i}\right)=v$；
- 那么进行BN归一化时$x_{n o r m a l}^{i}=\frac{x^{i}-E_{\text {mean }}}{\sqrt{V_{\text {mean }}+\varepsilon}}$应该代入$v$，但是实际我们代入的是训练阶段的滑动平均值，即$\frac{v+e^{2}}{p}-e^{2}$，这就是BN和Dropout一同使用会使BN的测试阶段发生**方差偏移（Variance Shift）**的现象；



**解决方案**

- 不使用Dropout，即 $p=1$；
- **在所有 BN 层后使用 Dropout，因为Dropout是带来方差偏移的根本原因；**

- **把Dropout改为一种更加稳定的形式（对方差不敏感）：**高斯Dropout、均匀分布Dropout；



------

**BN / LN / IN / GN / CmBN**

![image-20220828151017742](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828151017742.png)



**对特征图做归一化，**对于`[B,C,W,H]`这样的训练数据而言：

- **BN** 是在`[B,W,H]`维度求均值方差进行规范化 -> CNN              
- **LN** 是对`[C,W,H]`维度求均值方差进行规范化   -> RNN，Transformer
- **IN** 是对`[W,H]`维度求均值方差进行规范化  -> 图像风格化：GAN，style transfer
- **GN **先对通道进行分组，每个组内的所有 [$C_i$,W,H] 维度求均值方差进行规范化  - > 目标检测，语义分割等要求尽可能大分辨率任务；

![image-20220828151458791](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828151458791.png)

- **CBN **将前k-1个iteration的样本参与当前均值和方差的计算；

![image-20220828150551417](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828150551417.png)



------

#### 数据增强

```
 - Mix up / Cutout / Mosaic
 - Label Smoothing
 - 物体的复制粘贴（小物体）
 - 随机剪裁，翻转，缩放，亮度，色调，饱和度
 - 对普通数码照片进行归一化可以简单的将0-255线性映射到0-1；而医学图像、遥感图像由于白噪声的存在则不能简单的归一化到0-1；
```

- 拼接增广指随机找几张图各取一部分或者缩小之后拼起来作为一幅图用，拼接出来的图有强烈的拼接痕迹；
- 抠洞指随机的将目标的一部分区域扣掉填充0值；

- **拼接、抠洞属于人为制造的伪显著区域，不符合实际情况，对工程应用来说没有意义，白白增加训练量；**
- **训练过程随机缩放也是没必要的，缩放之后的图像可能会导致特征图和输入图像映射错位；**



------

#### 类别不平衡



**数据**

- **过采样** （增加噪声）/ **降采样**（先对样本聚类，在需要降采样的样本上，按类别进行降采样）
- Tomek连接 / **SMOTE**（选择少数样本，对其k临近插值）
- **增加数据** / **使用多种重采样的训练集**



- **评估指标：**避免使用Accuracy，可以用confusion matrix，precision，recall，f1-score，AUC，ROC等指标
- **Focal Loss**（正负样例不平衡alpha，简单困难样例不平衡belta）-> 目前普遍存在一个误解：认为focal loss是解决样本不均衡的杀器，实际上更重要的是**分类层bias的初始化**(yolox和v5都用了），另外在300Epochs训练后也可以解决不均衡问题



------

#### Self-Attention

- CNN输入维度固定，不能处理可变向量；
- 一组向量作为输入；





## ########################################



## 目标检测



------

#### Label Assignment

- RetinaNet根据**Anchor和目标的IoU**来确定正负样本；
- FCOS根据**目标中心区域和目标的尺度**确定正负样本；



Assign算法的原则：

- 中心先验：FCOS / CenterNet
- Loss aware（动态匹配）：FreeAnchor / ATSS
- 不同目标设定不同数量正样本（进一步动态）：PAA / AutoAssign
- 全局信息：IQDet / OTA



------

#### ROI Pooling & Align

 

**两次整数化（量化）过程**：

- **region proposal**的xywh通常是小数，但是为了方便操作会把它整数化；
- 将整数化后的边界区域**平均分割成 k x k 个单元**，对每一个单元边界进行整数化；

**经过上述两次整数化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度；**  - >  **mis-alignment**



**ROI Align**: **取消量化操作**：

- 遍历每一个候选区域，保持浮点数边界不做量化，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而**将整个特征聚集过程转化为一个连续的操作**；
- 将候选区域分割成k x k个单元，每个单元的边界也不做量化，在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作；



------

#### Anchor-Base VS Anchor-free



**Anchor-Based：**

- 检测性能**对于anchor的大小，数量，长宽比都非常敏感**，这些固定的anchor极大地**损害了检测器的泛化性**，导致对于不同任务，其anchor都必须重新设置大小和长宽比；
- 为了去匹配真实框，需要生成大量的anchor，但是大部分的anchor在训练时标记为negative，所以就造成**正负样本的不平衡**；
- 在训练中，需要**计算所有anchor与真实框的IOU**，这样就会**消耗大量内存和时间**；



**Anchor-Free：**

- **语义模糊性**，即两个物体的中心点落在了同一个网格中 ：
    - FCOS默认将该点分配给面积最小的目标；
    - 使用FPN界定每个特征层的检测范围；
    - center sampling准则；【只有GT bbox中心附近的一定范围内的小bbox内的点，分类时才作为正样本】
- anchor free缺少先验知识，所以优化不如anchor based的方法**稳定**；



------

#### 网络的分类



- [x] **基于阶段：**

    - **多阶：**Casade RCNN

     - **两阶：**RCNN / Fast RCNN / Faster RCNN

     - **单阶：**SSD / YOLO v1~v5 / RetinaNet / EfficientNet / CornerNet / FCOS

- [x] **是否使用Anchor：**

    - **Anchor Free:**
        - **Dense Prediction：**DenseBox
        - **Keypoint-based：**CenterNet / CornerNet


      - **Anchor based：**
        - **Dimension Clusters：**YOLO v2 ~ YOLO v5 / PP-YOLO / EfficientNet 

        - **Hand pickeed：**SSD / Faster RCNN



- [x] **不同标签方案：**

    - **Region proposal-based：**RCNN / Fast RCNN / Faster RCNN
    - **基于keypoint-based：**CornerNet / CenterNet / RepPoints

    - **基于author-IoU：**SSD / Faster RCNN / YOLO v2 ~ v5 / EfficientNet 



------

#### 传统目标检测



**区域选择->特征提取->分类器**

- 使用不同尺度的滑动窗口选定图像的某一区域为候选区域；
- 从对应的候选区域提取如Harrs HOG等一类或者多类**特征**；
- 使用 SVM 等分类算法对对应的候选区域进行分类，判断是否属于待检测的目标；



**缺点：**

- 基于滑动窗口的区域选择策略没有针对性，**时间复杂度高，窗口冗余；**
- 手工设计的特征对于多样性的变化没有很好的**鲁棒性**；

 

------



## ########################################



## 重要代码



#### Focal_Loss

- **alpha:** (optional) Weighting factor in range (0,1) to balance **positive vs negative examples** or -1 for ignore. Default = 0.25；
- **gamma:** Exponent of the modulating factor (1 - p_t) to balance **easy vs hard examples；**

```python
import torch
import torch.nn.functional as F

from ..utils import _log_api_usage_once



[docs]def sigmoid_focal_loss(
    inputs: torch.Tensor,
    targets: torch.Tensor,
    alpha: float = 0.25,
    gamma: float = 2,
    reduction: str = "none",
):
    """
    Args:
        inputs: A float tensor of arbitrary shape.
                The predictions for each example.
        targets: A float tensor with the same shape as inputs. Stores the binary
                classification label for each element in inputs
                (0 for the negative class and 1 for the positive class).
        alpha: (optional) Weighting factor in range (0,1) to balance
                positive vs negative examples or -1 for ignore. Default = 0.25
        gamma: Exponent of the modulating factor (1 - p_t) to
               balance easy vs hard examples.
        reduction: 'none' | 'mean' | 'sum'
                 'none': No reduction will be applied to the output.
                 'mean': The output will be averaged.
                 'sum': The output will be summed.
    Returns:
        Loss tensor with the reduction option applied.
    """
    if not torch.jit.is_scripting() and not torch.jit.is_tracing():
        _log_api_usage_once(sigmoid_focal_loss)
        
    p = torch.sigmoid(inputs)
    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")
    p_t = p * targets + (1 - p) * (1 - targets)
    loss = ce_loss * ((1 - p_t) ** gamma)

    if alpha >= 0:
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        loss = alpha_t * loss

    if reduction == "mean":
        loss = loss.mean()
    elif reduction == "sum":
        loss = loss.sum()

    return loss
```



------





## ########################################
