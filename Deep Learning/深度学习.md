##  ########################################



**AI Performance = Data(70%) + Model(CNN、RNN、Transformer、Bert、GPT 20%) + Trick(loss、optimizer、etc 10%)** 



####  调参经验

```
网络：
 1.Backbone和Heads的不同学习率: 因为Backbone和Heads在结构上的差异，使用不同的学习率是可以有效的使得网络达到更好, 更稳定的收敛效果


数据：
 1.数据归一化: 由于白噪声的存在，医学图像、遥感图像则不能简单的归一化到0-1，普通数码照片可以简单的将0-255线性映射到0-1进行归一化
 2.cv2读取图片速度快比Pillow快
 3.Callback: 自己给自己的测试集打Label放进训练


train.py
 01. DropOut2d / DropPath / DropBlock (block size控制大小最好在7x7，keep prob在整个训练过程中从1逐渐衰减到指定阈值)
 02. Batch Normalization / Group Normalization (每组channel为16)
 03. Focal Loss: 对CE loss增加了一个调制系数来降低容易样本的权重值，使得训练过程更加关注困难样本
 04. 多loss加权混合: 保证组合后的损失函数下降不平衡导致的损失函数的倾斜化
 05. OneCycleLR + SGD / Adam （torch.optim.lr_scheduler.ReduceLROnPlateau）
 06. L1 L2正则化 == weight decany
 07. 混合精度FP16: 加快训练速度，提高训练精度
 08. 加速训练pin_memory=true work_numbers=x (卡的数量x4)，data.to(device, no_blocking=True)设置为True后数据直接保存在锁页内存中，后续直接传入cuda，否则需要先从虚拟内存中传入锁页内存中再传入cuda
 09. Warm Up / Early stopping
 10. TTA: 对于测试数据集进行不同方向的预测后将预测的模型进行组合，在不改变模型内部参数的情况下，对效果进行提升 (增加推理时间)
 11. seed（42）随即种子数设置为42
 12. 3x3卷积有利于保持图像性质
 13. ReLU可使用inplace操作减少显存消耗
 14. Label Smoothing: 使得原本的hard-target变为soft-target，让标签分布的熵增大,使网络优化更加平滑,通常用于减少训练的过拟合问题并进一步提高分类性能
 15. With Flooding: 当training loss大于一个阈值时，进行正常的梯度下降；当training loss低于阈值时，会反过来进行梯度上升，让training loss保持在一个阈值附近，让模型持续进行"random walk"
 16.权重初始化时可使用He初始化, 但是更为重要的是对应权重正负号, 值没有那么重要
```



#### 深度学习挑战

- **应该更多地关注边缘情况（也就是异常值，或不寻常的情况），并思考这些异常值对预测可能意味着什么：**我们手上有大量的关于日常事务的数据，当前的技术很容易处理这些数据；而对于罕见的事件，我们得到的数据非常少，且目前的技术很难去处理这些数据；
- **我们人类拥有大量的不完全信息推理的技巧，也许可以克服生活中的长尾问题：**但对于目前流行的、更多依赖大数据而非推理的人工智能技术来说，长尾问题是一个非常严重的问题；
- 世上并不只有一种思维方式，因为思维并不是一个整体。相反，**思维是可以分为部分的，而且其不同部分以不同的方式运作：**例如，深度学习在识别物体方面做得相当不错，但在计划、阅读或语言理解方面做得差一些；
- **使用深度学习进行调试非常困难，因为没有人真正理解它是如何工作的，也没有人知道如何修复问题：**大众所知道的那种调试在经典编程环境中并不适用；



## ########################################



## 深度学习



------

#### 局部最小值 & 鞍点（Hessian矩阵）

- 求导，得到导数为0的点，并进行泰勒展开；
- 泰勒展开含二阶导数部分，求其Hessian矩阵及其对应特征值；
- 特征值全部大于0，则为局部最小值；特征值全部小于0，则为局部最大值；特征值有正有负，则为鞍点；

<img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826223004304.png" alt="image-20220826223004304" style="zoom: 50%;" />

<img src="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826223246364.png" alt="image-20220826223246364" style="zoom:50%;" />



```
**如果正定Hessian矩阵的特征值都差不多，那么梯度下降的收敛速度越快，反之如果其特征值相差很大，那么收敛速度越慢；**
```



一阶梯度下降方法中$\alpha$控制步长：
$$
x_{t+1}=x_{t}-\alpha g, \quad g=f^{\prime}\left(x_{t}\right)                 
$$
二阶方法将函数$f(\theta)$在局部极值点附$f(\theta^*)$近进行二阶Taylor展开，其中$g$为梯度向量，$H$为Hessian矩阵：
$$
f\left(\theta^{*}+\Delta \theta\right) \approx f\left(\theta^{*}\right)+g^{T} \Delta \theta+\frac{1}{2}(\Delta \theta)^{T} H(\Delta \theta)
$$
对上式求导并令其为0，以求在二阶近似原函数的情况下快速求出函数极值点：
$$
\Delta \theta=-H^{-1} g
\Rightarrow x_{t+1}=x_{t}-H^{-1} g, g=f^{\prime}\left(x_{t}\right)
$$
结合两个更新公式可知**Hessian矩阵的特征值控制了更新步长：**$\alpha = H^{-1}$

详细的，对实对称矩阵而言：$H=E \Lambda E^{T}$，其中$E=\left[e_{1} e_{2} \ldots e_{n}\right]$是单位特征向量矩阵，$\Lambda=\operatorname{diag}\left(\left[\lambda_{1} \lambda_{2} \ldots \lambda_{n}\right]\right)$是对应特征值对角矩阵，则：
$$
H^{-1} g=\left(E \Lambda E^{T}\right)^{-1} g=E \Lambda^{-1} E^{T} g=\sum_{i}^{n} \frac{e_{i}^{T} g}{\lambda_{i}} e_{i}
$$
可以看出，这**里控制步长的有对应的Hessian矩阵特征值**，极端的![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+g)则表示![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+%5CLeftrightarrow++1%2F%5Clambda_%7Bi%7D%2C+%5Cvee+i)这种，**若特征值间差异巨大，则有些方向学习缓慢，有些不断波动**（二维情况就是经常看到的那种蛇形曲线...)，这些现象也侧面说明了步长这东西；



------

#### Small Batch & Large Batch

![image-20220826224718603](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826224718603.png)

![image-20220826225430187](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826225430187.png)



------

#### 梯度下降 & 学习率



**一阶方法：**随机梯度下降（SGD）、动量（Momentum）、牛顿动量法（Nesterov动量）、AdaGrad（自适应梯度）、RMSProp（均方差传播）、Adam、Nadam

**二阶方法：**牛顿法、拟牛顿法、共轭梯度法（CG）、BFGS、L-BFGS

**自适应优化：**Adagrad（累积梯度平方）、RMSProp（累积梯度平方的滑动平均）、Adam（带动量的RMSProp，即同时使用梯度的一、二阶矩）

 

------



**一个框架来梳理所有的优化算法：**

- 首先定义待优化参数 $w$,   目标函数$f(w)$,   初始学习率 $\alpha_{\circ}$
- 之后开始进行迭代优化，在每个epoch $\boldsymbol{t}$ :
  - 计算目标函数关于当前参数的梯度： $g_{t}=\nabla f\left(w_{t}\right)$
  - 根据历史梯度计算一阶动量和二阶动量:
    $m_{t}=\phi\left(g_{1}, g_{2}, \cdots, g_{t}\right) ; V_{t}=\psi\left(g_{1}, g_{2}, \cdots, g_{t}\right)$
  - 计算当前时刻的下降梯度： $\eta_{t}=\alpha \cdot m_{t} / \sqrt{V_{t}}$
  - 根据下降梯度进行更新： $w_{t+1}=w_{t}-\eta_{t}$

------



**马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）**

- **基于动量**的方法使得最优化过程看起来像是一个球滚下山的样子
- **SGD**很难突破对称性，一直卡在顶部
- **RMSProp之类**的方法能够看到马鞍方向有很低的梯度（因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进）

------



**SGD（普通更新）**



最简单的沿着负梯度方向改变参数；假设有一个**参数向量x**及其**梯度dx**，那么最简单的更新的形式是：

```python
# 普通更新
x += - learning_rate * dx
```



- SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点； 
- $(\mathrm{W}, \mathrm{b})$ 的每一个分量获得的梯度绝对值有大有小,，一些情况下，将会迫使优化路径变成Z字形状；
- SGD求梯度的策略过于随机，由于上一次和下一次用的是完全不同的Batch数据,，将会出现优化的方向随机的情况；



------

**SGDM（动量更新，解决梯度随机性）**



该方法从**物理角度**上对于最优化问题得到的启发：

从本质上说，动量法就像我们从山上推下一个球，球在滚下来的过程中累积动量，变得越来越快（直到达到终极速度，如果有空气阻力的存在，则$\mu$<1）；同样的事情也发生在参数的更新过程中：**对于在梯度点处具有相同的方向的维度，其动量项增大，对于在梯度点处改变方向的维度，其动量项减小。**因此我们可以得到更快的收敛速度，同时可以减少摇摆。



**也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 mu的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向**；



在SGD中，梯度影响**位置**；

而在SGDM的更新中，物理观点建议**梯度只是影响速度**，然后**速度再影响位置**： 

```python
 # 动量更新
    v = mu * v - (1 - mu) * dx # 与速度融合，mu其物理意义与摩擦系数更一致
    x += v # 与位置融合
    
# mu通常取值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向
```

![image-20220826230230491](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826230230491.png)

![image-20220826230752974](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220826230752974.png)



------



**NAG（Nesterov动量）**



SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。



当参数向量位于某个位置 *x* 时，观察上面的动量更新公式，动量部分会通过$mu * v$改变参数向量；

因此，如要计算梯度，那么可以将**未来的近似位置**$ x+mu*v$ 看做是“**向前看**”，这个点在我们一会儿要停止的位置附近。因此，**计算** $ x+mu*v$**的梯度**而不是“旧”位置 *x* 的梯度，使用Nesterov动量，我们就在这个“向前看”的地方计算梯度

```python
x_ahead = x + mu * v
计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)
v = mu * v - learning_rate * dx_ahead
x += v  
```

上面的程序还得计算dx_ahead，通过对 x_ahead = x + mu * v 使用变量变换进行改写，然后用x_ahead而不是x来表示上面的更新，即：实际**存储**的参数向量总是**向前一步版本**。x_ahead 的公式（将其**重新命名为x**）就变成了：

```python
v_prev = v # 存储备份
v = mu * v - learning_rate * dx # 速度更新保持不变，mu=0.9
x += -mu * v_prev + (1 + mu) * v # 位置更新变了形式
```



------



**Adagrad**

![image-20220827213002839](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827213002839.png)



------



**RMSprop**



引用自Geoff Hinton的Coursera课程，具体说来，就是它使用了一个**梯度平方的滑动平均**：

```python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
```

decay_rate=0.9，learning_rate=0.001，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，但**其更新不会让学习率单调变小**； 

- 不累积全部历史梯度，而**只关注过去一段时间窗口的下降梯度**，而指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：
- 历史中梯度权重不同，最近一些梯度具有较大的影响；

![image-20220827213814881](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827213814881.png)



------



**Adam**



**Adam本质上实际是RMSProp+动量**：Adam对每一个参数都计算自适应的学习率：除了像RMSprop一样存储一个历史梯度平方的滑动平均$vt$，Adam同时还保存一个历史梯度的滑动平均$m_t$，类似于动量：

```python
# 根据历史梯度计算一阶动量和二阶动量
m_t = beta1*m + (1-beta1)*dx
v_t = beta2*v + (1-beta2)*(dx**2)

# 当mt和vt初始化为0向量时，发现它们都偏向于0，尤其是在初始化的步骤和当衰减率很小的时候（例如beta1和beta2趋向于1）,通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差
m_hat = m_t / 1 - (beta1 ** t）
v_hat = v_t / 1 - (beta2 ** t)

x += - learning_rate * m_hat / (np.sqrt(v_hat) + eps)  # eps=1e-8, beta1=0.9, beta2=0.999
```

![image-20220827214144011](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827214144011.png)



------



**学习率 Learning Rate**

![image-20220827214838018](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827214838018.png)



**Warm Up 需要在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 Decay：**$\sigma$表示历史统计数据，而在训练开始时这一数据并不准确，所以给它一个较小的学习率(0.001)让它在原地学习，之后数据会逐渐准确，可以将其增加(0.1)，之后使用常见的Learning Rate Decay；

![image-20220827215343432](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220827215343432.png)



------

#### Batch Normalization & Dropout



**解决Internal Covariate Shift**：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程；之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近，所以这**导致反向传播时低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**，**而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的分布；**



**优点：**

- **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度；**
- **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题；**
- **起到一定的正则化作用，防止过拟合；**
- **BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定；**



------



$Input: B=\left\{x_{1 \ldots m}\right\} ; \gamma, \beta($ parameters to be learned $)$

$\text { Output }:\left\{y_{i}=B N_{\gamma, \beta}\left(x_{i}\right)\right\} \\$
$$
\begin{array}{r}

\mu_{B} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\
\sigma_{B}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2} \\
\tilde{x}_{i} \leftarrow \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\epsilon}} \\
y_{i} \leftarrow \gamma \tilde{x}_{i}+\beta
\end{array}
$$

- **BN [B, H, W] 的精髓在于归一之后，使用$\gamma, \beta$作为还原参数，让数据尽可能保留原始的表达能力；**

```
批规范化（Batch Normalization，BN）：在 minibatch维度 上在每次训练iteration时对隐藏层进行归一化
标准化（Standardization）：对输入 数据 进行归一化
正则化（Regularization）：通常是指对 参数 在量级和尺度上做约束，缓和过拟合情况，L1 L2正则化
```



均值的计算，就是在一个批次内将每个通道中的数字单独加起来，再除以 $N*W*H$；举个例子：该批次内有10张图片，每张图片有三个通道RBG，每张图片的高、宽是H、W，那么均值就是计算**10张图片R通道的像素数值总和**除以$10*W*H$，再计算B通道全部像素值总和除以$N*W*H$，最后计算G通道的像素值总和除以$N*W*H$。方差的计算类似；

可训练参数 $\beta,\gamma$的维度等于**张量的通道数**，在上述例子中，RBG三个通道分别需要一个 $\beta和一个\gamma$，所以$\vec{\gamma}, \vec{\beta}$的维度等于3；



------

**基本理论**

![image-20220828125737750](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828125737750.png)

在未进行Normalization前所有的数据都相互独立，但是经过Normalization会使得各个数据间相互关联，每一个数据的改变会改变$\mu和\sigma$进而改变后一层数据；

![image-20220828130410672](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828130410672.png)

$\beta和\gamma$为可学习参，用于还原原始参数分布；

![image-20220828130924049](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828130924049.png)

![image-20220828131321546](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828131321546.png)

![image-20220828133459231](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828133459231.png)

------

**PyTorch中BN**



在PyTorch中**将gamma和beta改叫weight、bias**，使得打印网络参数时候只会打印出weight和bias（PyTorch中只有可学习的参数才称为Parameter）,但是`Net.state_dict()`是有running_mean和running_var的，**因为running_mean和running_var不是可以学习的变量，只是训练过程对很多batch的数据统计;**



BN层的**输出Y与输入X之间的关系**：**Y = (X - running_mean) / sqrt(running_var + eps) * gamma + beta**，其中**gamma、beta为可学习参数（在PyTorch中分别改叫weight和bias），训练时通过反向传播更新**；而**running_mean、running_var则是在前向时先由X计算出mean和var，再由mean和var以动量momentum来更新running_mean和running_var**，所以**在训练阶段，running_mean和running_var在每次前向时更新一次**；在**测试阶段，则通过`net.eval()`固定该BN层的running_mean和running_var，此时这两个值即为训练阶段最后一次前向时确定的值，并在整个测试阶段保持不变；**



**先更新running_mean和running_var，再计算BN；**

```
训练时：
running_mean = (1 - momentum) * running_mean + momentum * mean_cur
running_var = (1 - momentum) * running_var + momentum * var_cur
```

```
测试时：
running_mean = running_mean
running_var = running_var
```



------

**Conv和BN的融合**
$$
\begin{aligned}
y_{\text {conv }} &=w \cdot x+b \\
y_{b n} &=\gamma \cdot\left(\frac{y_{\text {conv }}-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
&=\gamma \cdot\left(\frac{w x+b-E[x]}{\sqrt{\operatorname{Var}[x]+\epsilon}}\right)+\beta \\
\hat{w} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot w \\
\hat{b} &=\frac{\gamma}{\sqrt{\operatorname{Var}[x]+\epsilon}} \cdot(b-E[x])+\beta \\
y_{b n} &=\hat{w} \cdot x+\hat{b}
\end{aligned}
$$

------

**Dropout**



`torch.nn.Dropout2d(p=0.5, inplace=False)`：input shape: (N, C, H, W)， output shape: (N, C, H, W)



**Dropout在层与层之间加噪声，是一种正则**；**在全连接使用，CNN用BN；**

Dropout 是**在训练过程中以一定的概率的使神经元失活，控制模型复杂度，提高模型的泛化能力，减少过拟合**，而在测试时，应该用整个训练好的模型，因此不需要Dropout；



- **Dropout 在训练时采用**，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险；而在测试时，应该用整个训练好的模型，因此**测试时不需要dropout**；

- **在测试时如果丢弃一些神经元，这会带来结果不稳定的问题：**给定一个测试数据，有时候输出a有时候输出b，结果不稳定，用户可能认为模型预测不准。那么**一种”补偿“的方案就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的**。比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，(1-p)的概率丢弃，那么它输出的期望是`p*x+(1-p)*0=px`，因此测试的时候把这个神经元的权重乘以p可以得到同样的期望；



------

**BN和Dropdout同时使用**



**方差偏移现象**

Dropout 与 Batch Normalization之间冲突的关键是**网络状态切换过程中存在神经方差的不一致行为，这种方差不匹配可能导致数值不稳定；而随着网络越来越深，最终预测的数值偏差可能会累计，从而降低系统的性能；**



- 假设某层单个神经元在某一Batch的Batch Normalization后输出的期望，方差分别为：$E\left(x_{i}\right)=e$,  $V\left(x_{i}\right)=v$；
- 对于Dropout，训练过程中的计算可以表示为：$x_{d}^{i}=\frac{x_{i}}{p} \delta_{i}$，其中$\delta_{i}$表示服从概率$p$伯努利分布采样的随机变量；
- 那么在训练阶段经过Dropout后，该Batch的期望和方差分别为：

$$
\begin{gathered}
E\left(x_{d}^{i}\right)=\frac{e}{p} * p+0 *(1-p)=e \\
V\left(x_{d}^{i}\right)=E\left((x_{d}^{i})^2\right)-E\left(x_{d}^{i}\right)^{2}=\frac{p *\left(V\left(x_{i}\right)+E\left(x_{i}\right)^{2}\right)}{p^{2}}-e^{2}=\frac{v+e^{2}}{p}-e^{2}
\end{gathered}
$$

​	 根据方差和期望的代数关系: $V(x)=E\left(x^{2}\right)-E(x)^{2}$ ：由于 $x_{d}^{i}=\frac{x^{i}}{p} * \delta_{i}$，即：
$$
E\left((x_{d}^{i})^2\right)=E\left(\left(\frac{x^{i}}{p} * \delta_{i}\right)^{2}\right)=\frac{E\left(\left(x^{i}\right)^{2}\right) * E\left(\delta_{i}^{2}\right)}{p^{2}}=\frac{\left(V\left(x^{i}\right)+E\left(x^{i}\right)^{2}\right) * p}{p^{2}}
$$

- 在Dropout测试阶段，$x_{d}^{i}=\delta_{i}$，不再根据采样概率$p$进行缩放，方差$V\left(x_{i}\right)=v$；
- 那么进行BN归一化时$x_{n o r m a l}^{i}=\frac{x^{i}-E_{\text {mean }}}{\sqrt{V_{\text {mean }}+\varepsilon}}$应该代入$v$，但是实际我们代入的是训练阶段的滑动平均值，即$\frac{v+e^{2}}{p}-e^{2}$，这就是BN和Dropout一同使用会使BN的测试阶段发生**方差偏移（Variance Shift）**的现象；



**解决方案**

- 不使用Dropout，即 $p=1$；
- **在所有 BN 层后使用 Dropout，因为Dropout是带来方差偏移的根本原因；**

- **把Dropout改为一种更加稳定的形式（对方差不敏感）：**高斯Dropout、均匀分布Dropout；



------

**BN / LN / IN / GN / CmBN**

![image-20220828151017742](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828151017742.png)



**对特征图做归一化，**对于`[B,C,W,H]`这样的训练数据而言：

- **BN** 是在`[B,W,H]`维度求均值方差进行规范化 -> CNN              
- **LN** 是对`[C,W,H]`维度求均值方差进行规范化   -> RNN，Transformer
- **IN** 是对`[W,H]`维度求均值方差进行规范化  -> 图像风格化：GAN，style transfer
- **GN **先对通道进行分组，每个组内的所有 [$C_i$,W,H] 维度求均值方差进行规范化  - > 目标检测，语义分割等要求尽可能大分辨率任务；

![image-20220828151458791](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828151458791.png)

- **CBN **将前k-1个iteration的样本参与当前均值和方差的计算；

![image-20220828150551417](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220828150551417.png)



------

#### Transformer -> LayerNorm



- 相比于稳定前向输入分布，反向传播时mean和variance计算引入的梯度更有用，**LN可以稳定反向传播的梯度**；
- LN特别适合处理变长数据，因为是对channel维度（NLP中hidden维度）做操作，和句子长度和batch大小无关；

```python
CV: [B, C, Dim(H,W)]
NLP:[B, C, Dim]

CV或者NLP中都是在C维度Norm（C x Dim），但是不同数据的Dim不同，因此Norm看起来也似乎不同；
在CNN中Dim固定（H x W），因此图2中Dim[H, W]度画满了；而在Transformer中由于是变长数据，因此Dim不同，只画了一层，看起来像CV中的IN，但是他们的数据是不同的；
```

![image-20220914225541398](%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.assets/image-20220914225541398.png)

- **由于CV与NLP数据特性不同**（NLP数据在前向和反向传播中，batch统计量与梯度不稳定），因此Transformer使用LayerNorm，在ViT中使用BatchNorm要在FFN中间添加BN（**可以加速20% VIT的训练，原因是FFN没有被Normalized**）；



------

#### Data Augmentation

```
 - Mix up / Cutout / Mosaic
 - Label Smoothing
 - 物体的复制粘贴（小物体）
 - 随机剪裁，翻转，缩放，亮度，色调，饱和度
 - 对普通数码照片进行归一化可以简单的将0-255线性映射到0-1；而医学图像、遥感图像由于白噪声的存在则不能简单的归一化到0-1；
```

- 拼接增广指随机找几张图各取一部分或者缩小之后拼起来作为一幅图用，拼接出来的图有强烈的拼接痕迹；
- 抠洞指随机的将目标的一部分区域扣掉填充0值；

- **拼接、抠洞属于人为制造的伪显著区域，不符合实际情况，对工程应用来说没有意义，白白增加训练量；**
- **训练过程随机缩放也是没必要的，缩放之后的图像可能会导致特征图和输入图像映射错位；**



------

#### 数据类别不平衡

```
正负样本不均衡 + 类别不平衡（长尾分布）
```

**数据**

- **过采样** （增加噪声）/ **降采样**（先对样本聚类，在需要降采样的样本上，按类别进行降采样）
- Tomek连接 / **SMOTE**（选择少数样本，对其k临近插值）
- **增加数据** / **使用多种重采样的训练集**



- **评估指标：**避免使用Accuracy，可以用confusion matrix，precision，recall，f1-score，AUC，ROC等指标
- **Focal Loss**（正负样例不平衡alpha，简单困难样例不平衡belta）-> 目前普遍存在一个误解：认为focal loss是解决样本不均衡的杀器，实际上更重要的是**分类层bias的初始化**(yolox和v5都用了），另外在300Epochs训练后也可以解决不均衡问题



------



#### 梯度消失 & 梯度爆炸



目前优化神经网络的方法都是根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要**链式法则（Chain Rule）**的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用；

而链式法则是一个**连乘的形式**，所以当层数越深的时候，**梯度将以指数形式传播**；梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显；在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的**梯度值接近0**或**特别大**，也就是**梯度消失**或**爆炸**；

```
0 梯度消失：
 - 深层网络，雅各比矩阵最大特征值小于1  
 - 激活函数（Sigmoid）

1 梯度爆炸：
 - 深层网络，雅各比矩阵最大特征值大于1  
 - Weights初始化值太大
```



**解决方案：**

```
- 深层网络：
	- 残差结构(乘法改成加法，固定梯度1)

- 激活函数、权重：
	- 合理的激活函数：ReLU
	- 合理的参数初始化(He,Xavier,让每层均值和方差保持一致)
	- 权重衰减

- 梯度：
	- 梯度归一化/梯度剪切 -> 让梯度值在合理范围内[1e-6, 1e3]
	- Batch Normalization

- 预训练 + 微调
```



------



#### 权重初始化



一种比较简单有效的方法：$\quad(\mathrm{W}, \mathrm{b})$ 初始化从区间 $\left(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}}\right)$均匀随机取值，**其中 $d$ 为 $（ \mathrm{~W}, \mathrm{~b}$ ） 所在层的神经元个数；**
可以证明：如果X服从正态分布, 均值0, 方差1，且各个维度无关, 而 $(\mathrm{W}, \mathrm{b})$ 是 $\left(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}}\right)$ 的均匀分布, 则 $W^{T} X+b$ 是均值为0, 方差为1/3的正态分布；



**Kaiming初始化：**

- **前向传播**的时候, 每一层的**卷积计算的方差为1**；
- **反向传播**的时候, 每一层的继续往**前传的梯度方差为1**(因为每层会有两个梯度的计算, 一个用来更新当前层的权重, 一个继续传播,用于前面层的梯度的计算)；
- `torch.nn.init.kaiming_normal_(layer.weight,mode='fan_out', nonlinearity='relu')`



------



#### NAN & INF



**INF：**数值太大、权重初始值太大 、LearningRate太大；

**NAN：**Not a Number，计算异常如：除数为0产生、log(0)、sqrt(-x)等；

 

**INF解决方案：**

- 损失出现INF，反向传播参数更新为NAN，可选用适合的激活函数，对NAN、INF进行Mask；
- 权重初始化时保证其具有较小的方差；
- 使用较小LearningRate，或使用学习率衰减；
- 梯度剪裁、正则化：防止梯度爆炸（梯度消失）；
- 训练数据中出现脏数据：输入数据或者标签里面存在NaN；



------



#### 数据集划分



**验证集要和训练集来自于同一个分布（shuffle数据集然后开始划分），测试集尽可能贴近真实数据**



- 通常80%为训练集，20%为测试集
- 当**数据量较小**时（万级别）的时候将训练集、验证集以及测试集划分为**6：2：2**；若是**数据量很大**，可以将训练集、验证集、测试集比例调整为**98：1：1**
- 当数据量很小时，可以采用**K折交叉验证**
- 刚开始的时候，用训练集训练，验证集验证，确定超参数和一些细节；在验证集调到最优后，再把验证集丢进来训练，在测试集上测试
- 划分数据集时可采用随机划分法（当样本比较均衡时），分层采样法（当样本分布极度不均衡时）



------







## ########################################



## 目标检测



------

#### Label Assignment

- RetinaNet根据**Anchor和目标的IoU**来确定正负样本；
- FCOS根据**目标中心区域和目标的尺度**确定正负样本；



Assign算法的原则：

- 中心先验：FCOS / CenterNet
- Loss aware（动态匹配）：FreeAnchor / ATSS
- 不同目标设定不同数量正样本（进一步动态）：PAA / AutoAssign
- 全局信息：IQDet / OTA



------

#### ROI Pooling & Align

 

**两次整数化（量化）过程**：

- **region proposal**的xywh通常是小数，但是为了方便操作会把它整数化；
- 将整数化后的边界区域**平均分割成 k x k 个单元**，对每一个单元边界进行整数化；

**经过上述两次整数化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度；**  - >  **mis-alignment**



**ROI Align**: **取消量化操作**：

- 遍历每一个候选区域，保持浮点数边界不做量化，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而**将整个特征聚集过程转化为一个连续的操作**；
- 将候选区域分割成k x k个单元，每个单元的边界也不做量化，在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作；



------

#### Anchor-Base VS Anchor-Free



**Anchor-Based：**

- 检测性能**对于anchor的大小，数量，长宽比都非常敏感**，这些固定的anchor极大地**损害了检测器的泛化性**，导致对于不同任务，其anchor都必须重新设置大小和长宽比；
- 为了去匹配真实框，需要生成大量的anchor，但是大部分的anchor在训练时标记为negative，所以就造成**正负样本的不平衡**；
- 在训练中，需要**计算所有anchor与真实框的IOU**，这样就会**消耗大量内存和时间**；



**Anchor-Free：**

- **语义模糊性**，即两个物体的中心点落在了同一个网格中 ：
    - FCOS默认将该点分配给面积最小的目标；
    - 使用FPN界定每个特征层的检测范围；
    - center sampling准则；【只有GT bbox中心附近的一定范围内的小bbox内的点，分类时才作为正样本】
- anchor free缺少先验知识，所以优化不如anchor based的方法**稳定**；



------

#### 网络的分类



- [x] **基于Stage：**

    - **多阶：**Casade RCNN

     - **两阶：**RCNN / Fast RCNN / Faster RCNN

     - **单阶：**SSD / YOLO v1~v5 / RetinaNet / EfficientNet / CornerNet / FCOS

- [x] **是否使用Anchor：**

    - **Anchor Free:**
        - **Dense Prediction：**DenseBox
        - **Keypoint-based：**CenterNet / CornerNet

      - **Anchor based：**
        - **Dimension Clusters：**YOLO v2 ~ YOLO v5 / PP-YOLO / EfficientNet 

        - **Hand pickeed：**SSD / Faster RCNN


- [x] **不同标签方案：**
    - **Region proposal-based：**RCNN / Fast RCNN / Faster RCNN
    - **基于keypoint-based：**CornerNet / CenterNet / RepPoints

    - **基于author-IoU：**SSD / Faster RCNN / YOLO v2 ~ v5 / EfficientNet 



------

#### 传统目标检测



**区域选择->特征提取->分类器**

- 使用不同尺度的滑动窗口选定图像的某一区域为候选区域；
- 从对应的候选区域提取如Harrs HOG等一类或者多类**特征**；
- 使用 SVM 等分类算法对对应的候选区域进行分类，判断是否属于待检测的目标；



**缺点：**

- 基于滑动窗口的区域选择策略没有针对性，**时间复杂度高，窗口冗余；**
- 手工设计的特征对于多样性的变化没有很好的**鲁棒性**；

 

------





## ########################################



## 重要代码



#### Focal_Loss

- **alpha:** (optional) Weighting factor in range (0,1) to balance **positive vs negative examples** or -1 for ignore. Default = 0.25；
- **gamma:** Exponent of the modulating factor (1 - p_t) to balance **easy vs hard examples；**

```python
import torch
import torch.nn.functional as F

from ..utils import _log_api_usage_once



[docs]def sigmoid_focal_loss(
    inputs: torch.Tensor,
    targets: torch.Tensor,
    alpha: float = 0.25,
    gamma: float = 2,
    reduction: str = "none",
):
    """
    Args:
        inputs: A float tensor of arbitrary shape.
                The predictions for each example.
        targets: A float tensor with the same shape as inputs. Stores the binary
                classification label for each element in inputs
                (0 for the negative class and 1 for the positive class).
        alpha: (optional) Weighting factor in range (0,1) to balance
                positive vs negative examples or -1 for ignore. Default = 0.25
        gamma: Exponent of the modulating factor (1 - p_t) to
               balance easy vs hard examples.
        reduction: 'none' | 'mean' | 'sum'
                 'none': No reduction will be applied to the output.
                 'mean': The output will be averaged.
                 'sum': The output will be summed.
    Returns:
        Loss tensor with the reduction option applied.
    """
    if not torch.jit.is_scripting() and not torch.jit.is_tracing():
        _log_api_usage_once(sigmoid_focal_loss)
        
    p = torch.sigmoid(inputs)
    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")
    p_t = p * targets + (1 - p) * (1 - targets)
    loss = ce_loss * ((1 - p_t) ** gamma)

    if alpha >= 0:
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        loss = alpha_t * loss

    if reduction == "mean":
        loss = loss.mean()
    elif reduction == "sum":
        loss = loss.sum()

    return loss
```



------





## ########################################
